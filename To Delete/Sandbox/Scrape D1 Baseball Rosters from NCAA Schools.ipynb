{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c53f07-44c5-4ad6-ae28-59e00b96386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL IMPORTS AND FUNCTIONS\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pyvirtualdisplay import Display\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from requests import Session\n",
    "from requests.exceptions import MissingSchema, InvalidURL, SSLError, RequestException, Timeout\n",
    "import numpy as np\n",
    "\n",
    "_HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "base_ncaa_url = 'https://www.ncaa.com'\n",
    "year = '2024'\n",
    "prior_year = str(int(year) - 1)  #Prior year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d69e107-e2ef-44c6-bca1-bd1143255133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_parts(list_of_urls):\n",
    "    new_list = []\n",
    "    for url in list_of_urls:\n",
    "        index = url.find('.com/')\n",
    "        if index != -1:\n",
    "            new_url = url[index+4:]  # +4 because len('.com') is 4\n",
    "            new_list.append(new_url)\n",
    "        else:\n",
    "            new_list.append(url)  # If '.com/' is not found, keep the original url\n",
    "    return new_list\n",
    "\n",
    "# FROM NCAA WEBSITE\n",
    "def get_all_school_media_links():\n",
    "    media_links_df = pd.DataFrame(columns=['school_name', 'official_school_website', 'twitter', 'facebook'])\n",
    "    counter = 1\n",
    "    #23 IS THE NUMBER OF PAGES IN SCHOOLS-INDEX\n",
    "    while counter <= 23:\n",
    "        ncaa_schools_url = base_ncaa_url+'/schools-index/'+str(counter)\n",
    "        with Session() as s:\n",
    "            r = s.get(ncaa_schools_url, headers=_HEADERS, timeout = 5)\n",
    "        if r.status_code == 403:\n",
    "            print('An error occurred with the GET Request')\n",
    "            print('403 Error: blocked request')\n",
    "            continue\n",
    "        soup = BeautifulSoup(r.text, features='lxml')\n",
    "        # Find all 'a' tags in the table\n",
    "        table = soup.find('table', {'class': 'responsive-enabled'})\n",
    "        links = table.find_all('a')\n",
    "        # Extract the URLs and the school names from each 'a' tag\n",
    "        urls_and_names = [(link.get('href'), link.text) for link in links]\n",
    "        \n",
    "        for url, name in urls_and_names:\n",
    "            individual_school_url = base_ncaa_url+url\n",
    "            with Session() as s:\n",
    "                r = s.get(individual_school_url, headers=_HEADERS, timeout = 5)\n",
    "            if r.status_code == 403:\n",
    "                print('An error occurred with the GET Request')\n",
    "                print('403 Error: blocked request')\n",
    "            soup = BeautifulSoup(r.text, features='lxml')\n",
    "            # Find all 'a' tags in the div\n",
    "            div = soup.find('div', {'class': 'school-links'})\n",
    "            if div is None:\n",
    "                continue\n",
    "            media_links = div.find_all('a')\n",
    "        \n",
    "            # Extract the URLs from the 'href' attribute of each 'a' tag\n",
    "            media_urls = [media_link.get('href') for media_link in media_links]\n",
    "            # Add the URLs and the school name to the DataFrame\n",
    "            temp_df = pd.DataFrame({\n",
    "                'school_name': [name],\n",
    "                'official_school_website': [media_urls[0] if len(media_urls) > 0 else None],\n",
    "                'twitter': [media_urls[1] if len(media_urls) > 1 else None],\n",
    "                'facebook': [media_urls[2] if len(media_urls) > 2 else None]\n",
    "            })\n",
    "            if not name == '':\n",
    "                print(f\"got links for {name}\")\n",
    "                media_links_df = pd.concat([media_links_df, temp_df], ignore_index=True)\n",
    "            \n",
    "        counter+=1\n",
    "        print(f\"finished scraping page {counter}\")\n",
    "        # print(media_links_df.tail)\n",
    "    \n",
    "    # Drop duplicates after all pages have been processed\n",
    "    media_links_df = media_links_df.drop_duplicates()\n",
    "    return media_links_df\n",
    "\n",
    "def get_player_name_from_player_bio(soup):\n",
    "    try:\n",
    "        name_span = soup.find('span', {'class': 'sidearm-roster-player-name'})\n",
    "        \n",
    "        # Try to find the first name span\n",
    "        first_name_span = name_span.find('span', {'class': 'sidearm-roster-player-first-name'})\n",
    "        if first_name_span is not None:\n",
    "            first_name = first_name_span.text\n",
    "        else:\n",
    "            # Try something else if the first name span doesn't exist\n",
    "            spans = name_span.find_all('span')\n",
    "            first_name = spans[0].text if spans else ''\n",
    "        \n",
    "        # Try to find the last name span\n",
    "        last_name_span = name_span.find('span', {'class': 'sidearm-roster-player-last-name'})\n",
    "        if last_name_span is not None:\n",
    "            last_name = last_name_span.text\n",
    "        else:\n",
    "            # Try something else if the last name span doesn't exist\n",
    "            spans = name_span.find_all('span')\n",
    "            last_name = spans[1].text if len(spans) > 1 else ''\n",
    "        \n",
    "        name = f\"{first_name} {last_name}\"\n",
    "    \n",
    "    except AttributeError:\n",
    "        name = ''  # Handle missing name\n",
    "    \n",
    "    return name\n",
    "\n",
    "def get_jersey_number_from_player_bio(soup):\n",
    "    try:\n",
    "        jersey_number_span = soup.find('span', {'class': 'sidearm-roster-player-jersey-number'})\n",
    "        jersey_number_text = jersey_number_span.text.strip()\n",
    "        # Use regex to find the first number in the text\n",
    "        match = re.search(r'\\d+', jersey_number_text)\n",
    "        if match:\n",
    "            jersey_number = match.group()  # The first number in the text\n",
    "        else:\n",
    "            jersey_number = ''  # No number found in the text\n",
    "    except AttributeError:\n",
    "        jersey_number = ''  # Handle missing jersey number\n",
    "    return jersey_number\n",
    "\n",
    "def get_miscellaneous_player_bio_data(soup):\n",
    "    try:\n",
    "        ul = soup.find('ul', {'class': 'flex flex-item-1 row flex-wrap'})\n",
    "        dl_tags = ul.find_all('dl')\n",
    "        headers = [tag.find('dt').text.rstrip(':') for tag in dl_tags]  # Remove colons\n",
    "        data = [tag.find('dd').text for tag in dl_tags]\n",
    "    except AttributeError:\n",
    "        data = '' # Handle missing player bio info\n",
    "    return headers, data\n",
    "\n",
    "\n",
    "def get_social_media_link(soup):\n",
    "    try:\n",
    "        social_media_div = soup.find('div', {'class': 'sidearm-roster-player-social flex flex-wrap'})\n",
    "        if social_media_div is not None:\n",
    "            social_media_a = social_media_div.find('a', {'class': 'sidearm-roster-player-social-link'})\n",
    "            if social_media_a is not None:\n",
    "                social_media_link = social_media_a['href']\n",
    "            else:\n",
    "                social_media_link = ''  # Handle missing social media a tag\n",
    "        else:\n",
    "            social_media_link = ''  # Handle missing social media div\n",
    "    except KeyError:\n",
    "        social_media_link = ''  # Handle missing href attribute\n",
    "\n",
    "    return social_media_link\n",
    "\n",
    "def get_roster_soup(base_url, year, prior_year, _HEADERS):\n",
    "    if base_url.startswith(\"http:\"):\n",
    "        base_url = base_url.replace(\"http:\", \"https:\", 1)\n",
    "    if base_url.endswith(\"/\"):\n",
    "        base_url = base_url[:-1]\n",
    "    urls = [\n",
    "        base_url+'/sports/baseball/roster/'+year,\n",
    "        base_url+'/baseball/roster/'+year,\n",
    "        base_url+'/sports/baseball/roster/',\n",
    "        base_url+'/baseball/roster/',\n",
    "        base_url+'/sport/m-basebl/roster/',\n",
    "        base_url+'/sports/bsb/'+prior_year+'-'+year+'/roster'\n",
    "    ]\n",
    "    check_urls = [\n",
    "        '/sports/baseball/roster/'+year,\n",
    "        '/baseball/roster/'+year,\n",
    "        '/sports/baseball/roster/',\n",
    "        '/baseball/roster/',\n",
    "        '/sport/m-basebl/roster/',\n",
    "        '/sports/bsb/'+prior_year+'-'+year+'/roster'\n",
    "    ]\n",
    "    for url, check_url in zip(urls, check_urls):\n",
    "        try:\n",
    "            with Session() as s:\n",
    "                r = s.get(url, headers=_HEADERS, timeout=5)\n",
    "            if not r.url.endswith(check_url) and not r.url.endswith(check_url+'/'):  # If final URL does not end with check_url, a redirect occurred\n",
    "                print('Redirect occurred for:', url,'Didnt match:', r.url)\n",
    "                continue\n",
    "            elif r.status_code == 404:\n",
    "                print('404 Error')\n",
    "                continue\n",
    "            elif r.status_code == 403:\n",
    "                print('An error occurred with the GET Request')\n",
    "                print('403 Error: blocked request')\n",
    "                continue\n",
    "            elif r.status_code // 100 == 3:  # If status code is in 300s, it's a redirect\n",
    "                print('Redirect occurred with the GET Request for URL:', url)\n",
    "                continue \n",
    "            else:\n",
    "                soup = BeautifulSoup(r.text, 'lxml')\n",
    "                print(url)\n",
    "                return soup\n",
    "        except (MissingSchema, InvalidURL, SSLError, RequestException, Timeout) as e:\n",
    "            print('Broken URL:', url)\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_player_bio_links(soup):\n",
    "    links = []\n",
    "    if len(soup.find_all('div', {'class': 'sidearm-roster-player-image'})) == 0:\n",
    "        print('grabbing bio urls using find all a tags')\n",
    "        #might build nested if statements if this method doesn't work either\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and '/roster/' in href:\n",
    "                links.append(href)\n",
    "        links = remove_leading_parts(links)\n",
    "        return list(set(links)) \n",
    "    else:\n",
    "        print('grabbing bio urls within sidearm-roster-player-image')\n",
    "        for div in soup.find_all('div', {'class': 'sidearm-roster-player-image'}):\n",
    "            a = div.find('a')\n",
    "            href = a['href']\n",
    "            links.append(href)\n",
    "        \n",
    "def get_player_bio_soup(bio_url, base_url, _HEADERS):\n",
    "    player_bio_url = base_url + bio_url\n",
    "    print(player_bio_url)\n",
    "    try:\n",
    "        with Session() as s:\n",
    "            r = s.get(player_bio_url, headers=_HEADERS, timeout = 5)\n",
    "        if r.status_code == 403:\n",
    "            print('An error occurred with the GET Request')\n",
    "            print('403 Error: blocked request')\n",
    "            return None\n",
    "    except (MissingSchema, InvalidURL, SSLError, RequestException, Timeout) as e:\n",
    "        print('broken URL:', player_bio_url)\n",
    "        return None\n",
    "    soup = BeautifulSoup(r.text, features='lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "#FUTURE WORK CAN REPLACE THIS WITH A FUNCTION\n",
    "# with Session() as s:\n",
    "#         r = s.get(player_bio_url, headers=_HEADERS)\n",
    "#     if r.status_code == 403:\n",
    "#         print('An error occurred with the GET Request')\n",
    "#         print('403 Error: blocked request')\n",
    "#         return None\n",
    "\n",
    "#     soup = BeautifulSoup(r.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34bc507c-e677-480c-a57e-a72383da7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url: http://gozips.com\n",
      "https://gozips.com/sports/baseball/roster/2024\n",
      "grabbing bio urls using find all a tags\n",
      "['/sports/baseball/roster/dawson-tourney/8900', '/sports/baseball/roster/emmett-gillies/8762', '/sports/baseball/roster/james-strom/8907', '/sports/baseball/roster/ben-brombaugh/8901', '/sports/baseball/roster/spencer-atkins/8749', '/sports/baseball/roster/nicky-sackett/8903', '/sports/baseball/roster/michael-sprockett/8778', '/sports/baseball/roster/josiah-ross/8914', '/sports/baseball/roster/andrew-horvath/8764', '/sports/baseball/roster/michael-orlowski/8917', '/sports/baseball/roster/evan-bottone/8905', '/sports/baseball/roster/charlie-schebler/8896', '/sports/baseball/roster/jared-schaeffer/8918', '/sports/baseball/roster/yassir-kahook/8767', '/sports/baseball/roster/ryan-brown/8909', '/sports/baseball/roster/jack-poist/8773', '/sports/baseball/roster/anthony-fett/8759', '/sports/baseball/roster/reece-sutphin/8904', '/sports/baseball/roster/nick-kemper/8912', '/sports/baseball/roster/brett-dietrich/8757', '/sports/baseball/roster/jack-kelley/8916', '/sports/baseball/roster/fisher-pyatt/8915', '/sports/baseball/roster/elijah-griffith/8913', '/sports/baseball/roster/breydan-cavey/8755', '/sports/baseball/roster/charlie-rhee/8906', '/sports/baseball/roster/john-allen/8747', '/sports/baseball/roster/austin-lafferty/8898', '/sports/baseball/roster/max-bowman/8753', '/sports/baseball/roster/tre-rex/8895', '/sports/baseball/roster/parker-munday/8899', '/sports/baseball/roster/jacob-beall/8750', '/sports/baseball/roster/sean-perkins/8771', '/sports/baseball/roster/henry-hayman/8910', '/sports/baseball/roster/ian-pennington/8770', '/sports/baseball/roster/sam-seeker/8777']\n",
      "http://gozips.com/sports/baseball/roster/dawson-tourney/8900\n",
      " Retrieving roster information from: /sports/baseball/roster/dawson-tourney/8900\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/emmett-gillies/8762\n",
      " Retrieving roster information from: /sports/baseball/roster/emmett-gillies/8762\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/james-strom/8907\n",
      " Retrieving roster information from: /sports/baseball/roster/james-strom/8907\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/ben-brombaugh/8901\n",
      " Retrieving roster information from: /sports/baseball/roster/ben-brombaugh/8901\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/spencer-atkins/8749\n",
      " Retrieving roster information from: /sports/baseball/roster/spencer-atkins/8749\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/nicky-sackett/8903\n",
      " Retrieving roster information from: /sports/baseball/roster/nicky-sackett/8903\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/michael-sprockett/8778\n",
      " Retrieving roster information from: /sports/baseball/roster/michael-sprockett/8778\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/josiah-ross/8914\n",
      " Retrieving roster information from: /sports/baseball/roster/josiah-ross/8914\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/andrew-horvath/8764\n",
      " Retrieving roster information from: /sports/baseball/roster/andrew-horvath/8764\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/michael-orlowski/8917\n",
      " Retrieving roster information from: /sports/baseball/roster/michael-orlowski/8917\n",
      "skipped player\n",
      "http://gozips.com/sports/baseball/roster/evan-bottone/8905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m visited_bio_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m player_bio_url \u001b[38;5;129;01min\u001b[39;00m all_player_bio_links:\n\u001b[0;32m---> 49\u001b[0m     player_bio_soup \u001b[38;5;241m=\u001b[39m get_player_bio_soup(player_bio_url, base_url, _HEADERS)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m player_bio_soup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskipped player b/c bio_soup returned none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[70], line 208\u001b[0m, in \u001b[0;36mget_player_bio_soup\u001b[0;34m(bio_url, base_url, _HEADERS)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Session() \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m--> 208\u001b[0m         r \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mget(player_bio_url, headers\u001b[38;5;241m=\u001b[39m_HEADERS, timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn error occurred with the GET Request\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    792\u001b[0m     conn,\n\u001b[1;32m    793\u001b[0m     method,\n\u001b[1;32m    794\u001b[0m     url,\n\u001b[1;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1097\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1101\u001b[0m         (\n\u001b[1;32m   1102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1108\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_time_off:\n\u001b[1;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    635\u001b[0m         (\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[0;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    643\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    644\u001b[0m     cert_reqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_reqs,\n\u001b[1;32m    645\u001b[0m     ssl_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version,\n\u001b[1;32m    646\u001b[0m     ssl_minimum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_minimum_version,\n\u001b[1;32m    647\u001b[0m     ssl_maximum_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_maximum_version,\n\u001b[1;32m    648\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs,\n\u001b[1;32m    649\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir,\n\u001b[1;32m    650\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_data,\n\u001b[1;32m    651\u001b[0m     cert_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file,\n\u001b[1;32m    652\u001b[0m     key_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_file,\n\u001b[1;32m    653\u001b[0m     key_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_password,\n\u001b[1;32m    654\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    655\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context,\n\u001b[1;32m    656\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    657\u001b[0m     assert_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_hostname,\n\u001b[1;32m    658\u001b[0m     assert_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fingerprint,\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39mis_verified\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:783\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    781\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 783\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    784\u001b[0m     sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    785\u001b[0m     keyfile\u001b[38;5;241m=\u001b[39mkey_file,\n\u001b[1;32m    786\u001b[0m     certfile\u001b[38;5;241m=\u001b[39mcert_file,\n\u001b[1;32m    787\u001b[0m     key_password\u001b[38;5;241m=\u001b[39mkey_password,\n\u001b[1;32m    788\u001b[0m     ca_certs\u001b[38;5;241m=\u001b[39mca_certs,\n\u001b[1;32m    789\u001b[0m     ca_cert_dir\u001b[38;5;241m=\u001b[39mca_cert_dir,\n\u001b[1;32m    790\u001b[0m     ca_cert_data\u001b[38;5;241m=\u001b[39mca_cert_data,\n\u001b[1;32m    791\u001b[0m     server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    792\u001b[0m     ssl_context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    793\u001b[0m     tls_in_tls\u001b[38;5;241m=\u001b[39mtls_in_tls,\n\u001b[1;32m    794\u001b[0m )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:471\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py:515\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    512\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[1;32m    518\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[1;32m    519\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[1;32m    520\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[1;32m    521\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[1;32m    522\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[1;32m    523\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    524\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[1;32m    525\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##START WITH DATAFRAME OF ALL SCHOOL SPORT WEBSITES AND SCRAPE PLAYER BIO DATA FOR ALL PLAYERS ON THAT ROSTER\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "#This comes from the script Get All D1 School Website URL's\n",
    "official_websites = pd.read_csv('test_data/all_skipped_rosters_3_13.csv')\n",
    "official_websites = official_websites.dropna(subset=['school_website_url'])\n",
    "# test_websites = official_websites[official_websites['school_website_url']=='http://appstatesports.com']\n",
    "# print(test_websites)\n",
    "\n",
    "player_bio_df = pd.DataFrame()\n",
    "skipped_rosters = []\n",
    "skipped_players = []\n",
    "\n",
    "#Loop through all schools on NCAA website and try a url to retrieve their roster\n",
    "for index, base_url in enumerate(official_websites['school_website_url']):\n",
    "# for index, base_url in enumerate(test_websites['school_website_url']):\n",
    "    \n",
    "    print(f\"base_url: {base_url}\")\n",
    "    if base_url is None:\n",
    "        print('skipped roster')\n",
    "        skipped_rosters.append(base_url)\n",
    "        continue\n",
    "    roster_soup = get_roster_soup(base_url, year, prior_year, _HEADERS)\n",
    "    if roster_soup is None:  \n",
    "        print('skipped roster b/c roster page had no soup')\n",
    "        skipped_rosters.append(base_url)\n",
    "        continue    \n",
    "    all_player_bio_links = get_player_bio_links(roster_soup)\n",
    "    print(all_player_bio_links)\n",
    "\n",
    "    #CREATE FUNCTION GET_PLAYER_BIO_LINKS_USING_SIDEARM_ROSTER_PLAYER_IMAGE\n",
    "    #CREATE FUNCTION GET_PLAYER_BIO_LINKS_CATCH_ALL\n",
    "    #REBUILD GET_PLAYER_BIO_SOUP TO TAKE IN LIST OF URLS AND RETURN SOUP\n",
    "\n",
    "    # if len(roster_soup.find_all('div', {'class': 'sidearm-roster-player-image'})) == 0:\n",
    "    #     print('skipped roster b/c there was no div class sidearm-roster-player-image')\n",
    "    #     skipped_rosters.append(base_url)\n",
    "    #     continue    \n",
    "    # Need to do some sort of coalesce(\n",
    "    #s-person-details__personal-single-line s-text-paragraph-bold flex items-center gap-2\n",
    "    # sidearm-roster-template-list\n",
    "    \n",
    "    visited_bio_urls = set()\n",
    "    \n",
    "    for player_bio_url in all_player_bio_links:\n",
    "\n",
    "        player_bio_soup = get_player_bio_soup(player_bio_url, base_url, _HEADERS)\n",
    "        if player_bio_soup is None:\n",
    "            print('skipped player b/c bio_soup returned none')\n",
    "            skipped_players.append(player_bio_url)\n",
    "            continue\n",
    "        if player_bio_url in visited_bio_urls or 'roster/coaches/' in player_bio_url or '/roster/staff/' in player_bio_url:\n",
    "            print('on to next roster')\n",
    "            break\n",
    "            \n",
    "        print(f\" Retrieving roster information from: {player_bio_url}\")\n",
    "        visited_bio_urls.add(player_bio_url)\n",
    "        \n",
    "        name = get_player_name_from_player_bio(player_bio_soup)\n",
    "        if name == '':\n",
    "            print('skipped player')\n",
    "            skipped_players.append(player_bio_url)\n",
    "            continue\n",
    "        jersey_number = get_jersey_number_from_player_bio(player_bio_soup)\n",
    "        social_media_link = get_social_media_link(player_bio_soup)\n",
    "        headers, data = get_miscellaneous_player_bio_data(player_bio_soup)     \n",
    "        \n",
    "        player_data = {'Team': official_websites.loc[index, 'school'], 'Name': name, 'Jersey Number': jersey_number, 'Social Media': social_media_link}\n",
    "        player_data.update(dict(zip(headers, data)))\n",
    "        player_df = pd.DataFrame([player_data])      \n",
    "        player_bio_df = pd.concat([player_bio_df, player_df], ignore_index=True, sort=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "#MAKE SURE ALL SITES ARE SCRAPED. IF NOT SCRAPED FIND OUT WHY\n",
    "#NEED TO SEE IF USING LIST METHOD GET_PLAYER_BIO_LINKS IS MORE SUITABLE THEN roster_soup.find_all('div', {'class': 'sidearm-roster-player-image'})\n",
    "#MIGHT BE A COMBO OF THE TWO\n",
    "    #Add check to ensure all rosters were successfully scraped\n",
    "#IF ROSTER IS VALID BUT FOR SOME REASON SCRAPING DOESN'T WORK ADD TO A LIST TO KEEP REPEATING THROUGH UNTIL LIST COMPLETE\n",
    "#NEED TO GRAB NAME & JERSEY NUMBER FROM NCAA.STATS TEAM PAGE TO MATCH WITH BOX SCORES\n",
    "\n",
    "\n",
    "#CONSIDER SCRAPING DIRECTLY FROM ROSTER INSTEAD OF GOING TO INDIVIDUAL PLAYER BIOS\n",
    "#CONSIDER MAKING THIS A GENERATOR FUNCTION EVENUTALLY\n",
    "#NEED TO ADD TRANSFORMATION TO TIE TOGETHER ALL COLUMNS THAT ARE ACTUALLY THE SAME. (PARSE HT/WT TO SEPERATE COLUMNS, DELETE UNECESSARY COLUMNS, ADD URL'S TO SOCIAL MEDIA SITES)\n",
    "#NEED TO CREATE PROPER KEY TO JOIN TO OTHER TABLES (REMOVE NICKNAMES LAWRENCE \"Q\" NOBLE) AND TURN INTO NOBLE, LAWRENCE\n",
    "#IF JOIN BETWEEN NAMES IS NOT WORKING, CAN TRY TO GRAB JERSEY NUMBER FROM NCAA STATS AND JOIN ON THAT TO ROSTER+TEAM (MAY CREATE NEW TABLE OF ALL PLAYERS, JERSEY NUMBER AND WEBSITE NAME)\n",
    "#REORGANIZE AND TURN INTO DIFFERENT FILES AND DEFINITIONS\n",
    "#join box score stats to ncaa roster based on name and jersey number\n",
    "    #ALTERNATIVELY CAN GRAB TEAM ID FROM NCAA OR TRY TO GET CORRECT JERSEY NUMBER FROM NCAA STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63513dfd-f612-48b7-bbf4-d596e1f0f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skipped_rosters_df = pd.DataFrame(skipped_rosters)\n",
    "skipped_players_df = pd.DataFrame(skipped_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f8a85e5-a0ea-4f23-b21d-184e803ea076",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_bio_df.to_csv(f\"test_data/all_player_bios_3_13.csv\", index=False)\n",
    "skipped_rosters_df.to_csv('test_data/all_skipped_rosters_3_14.csv', index=False)\n",
    "skipped_players_df.to_csv('test_data/all_skipped_players_3_14.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7decab58-68cd-4c7f-9875-5c334c9d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SCRAPE SINGLE ROSTER FOR ALL PLAYER BIO LINKS AND DOWNLOAD PLAYER BIO DATA INTO DATAFRAME\n",
    "\n",
    "school_url = 'https://goheels.com/'\n",
    "with Session() as s:\n",
    "    r = s.get(school_url+'/sports/baseball/roster'+year, headers=_HEADERS)\n",
    "if r.status_code == 403:\n",
    "    print('An error occurred with the GET Request')\n",
    "    print('403 Error: blocked request')\n",
    "soup = BeautifulSoup(r.text, features='lxml')\n",
    "\n",
    "player_urls = []\n",
    "\n",
    "# Initialize an empty DataFrame with all possible column names to collect roster data\n",
    "# all_possible_columns = ['Name', 'Jersey Number', 'Social Media', 'Class', 'Position', 'Bat/Throw', 'B/T','Ht./Wt.', 'Hometown', 'High School','Major', 'Height','Weight','Year']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for div in soup.find_all('div', {'class': 'sidearm-roster-player-image'}):\n",
    "    a = div.find('a')\n",
    "    player_bio_url = a['href']\n",
    "    print(f\" Retrieving roster information from: {player_bio_url}\")\n",
    "    \n",
    "    with Session() as s:\n",
    "        r = s.get(school_url+'/sports/baseball/roster', headers=_HEADERS)\n",
    "    if r.status_code == 403:\n",
    "        print('An error occurred with the GET Request')\n",
    "        print('403 Error: blocked request')\n",
    "    soup = BeautifulSoup(r.text, features='lxml')\n",
    "\n",
    "    #START RETRIEVING DATA FROM PLAYER BIO WEBPAGE\n",
    "    try:\n",
    "        name_span = soup.find('span', {'class': 'sidearm-roster-player-name'})\n",
    "        \n",
    "        # Try to find the first name span\n",
    "        first_name_span = name_span.find('span', {'class': 'sidearm-roster-player-first-name'})\n",
    "        if first_name_span is not None:\n",
    "            first_name = first_name_span.text\n",
    "        else:\n",
    "            # Try something else if the first name span doesn't exist\n",
    "            spans = name_span.find_all('span')\n",
    "            first_name = spans[0].text if spans else ''\n",
    "        \n",
    "        # Try to find the last name span\n",
    "        last_name_span = name_span.find('span', {'class': 'sidearm-roster-player-last-name'})\n",
    "        if last_name_span is not None:\n",
    "            last_name = last_name_span.text\n",
    "        else:\n",
    "            # Try something else if the last name span doesn't exist\n",
    "            spans = name_span.find_all('span')\n",
    "            last_name = spans[1].text if len(spans) > 1 else ''\n",
    "        \n",
    "        name = f\"{first_name} {last_name}\"\n",
    "\n",
    "    except AttributeError:\n",
    "        name = ''  # Handle missing name\n",
    "    \n",
    "    # Get jersey number\n",
    "    try:\n",
    "        jersey_number_span = soup.find('span', {'class': 'sidearm-roster-player-jersey-number'})\n",
    "        jersey_number = jersey_number_span.text.strip()\n",
    "    except AttributeError:\n",
    "        jersey_number = ''  # Handle missing jersey number\n",
    "    \n",
    "    # Get player attributes\n",
    "    ul = soup.find('ul', {'class': 'flex flex-item-1 row flex-wrap'})\n",
    "    dl_tags = ul.find_all('dl')\n",
    "    headers = [tag.find('dt').text.rstrip(':') for tag in dl_tags]  # Remove colons\n",
    "    # print(f\"headers: {headers}\")\n",
    "    data = [tag.find('dd').text for tag in dl_tags]\n",
    "    \n",
    "    # Get social media link\n",
    "    try:\n",
    "        social_media_div = soup.find('div', {'class': 'sidearm-roster-player-social flex flex-wrap'})\n",
    "        if social_media_div is not None:\n",
    "            social_media_a = social_media_div.find('a', {'class': 'sidearm-roster-player-social-link'})\n",
    "            if social_media_a is not None:\n",
    "                social_media_link = social_media_a['href']\n",
    "            else:\n",
    "                social_media_link = ''  # Handle missing social media a tag\n",
    "        else:\n",
    "            social_media_link = ''  # Handle missing social media div\n",
    "    except KeyError:\n",
    "        social_media_link = ''  # Handle missing href attribute\n",
    "    \n",
    "    \n",
    "    # Add name, jersey number, and social media link to the list of headers and data\n",
    "    headers = ['Name', 'Jersey Number', 'Social Media'] + headers\n",
    "    data = [name, jersey_number, social_media_link] + data\n",
    "    \n",
    "    # Create a dictionary with the player's data\n",
    "    player_data = {'Name': name, 'Jersey Number': jersey_number, 'Social Media': social_media_link}\n",
    "    player_data.update(dict(zip(headers, data)))\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    player_df = pd.DataFrame([player_data])\n",
    "    \n",
    "    # Append the player's data to the DataFrame\n",
    "    df = pd.concat([df, player_df], ignore_index=True, sort=False)\n",
    "    \n",
    "    # print(df.columns)\n",
    "    print(df)\n",
    "    # break #Temporarily only loop once to get all types of fields and different html structures\n",
    "\n",
    "\n",
    "# # print(player_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be39167-b11b-4431-98c3-30d27b25572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE SINGLE PLAYER BIO PAGE\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests import Session\n",
    "\n",
    "# Your player bio URL\n",
    "player_bio_url = 'https://clariongoldeneagles.com/sports/baseball/roster/kasey-shughart/9657'\n",
    "\n",
    "# Initialize an empty DataFrame with all possible column names\n",
    "all_possible_columns = ['Name', 'Jersey Number', 'Social Media', 'Class', 'Position', 'Bat/Throw', 'B/T','Ht./Wt.', 'Hometown', 'High School','Major', 'Height','Weight','Year']\n",
    "player_bio_df = pd.DataFrame(columns=all_possible_columns)\n",
    "\n",
    "# Create a session\n",
    "with Session() as s:\n",
    "    # Send a GET request to the player bio URL\n",
    "    r = s.get(player_bio_url, headers=_HEADERS)\n",
    "\n",
    "# Check the status code of the response\n",
    "if r.status_code == 403:\n",
    "    print('An error occurred with the GET Request')\n",
    "    print('403 Error: blocked request')\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "# print(soup)\n",
    "\n",
    "##__________________ GET PLAYER NAME\n",
    "\n",
    "#START RETRIEVING DATA FROM PLAYER BIO WEBPAGE\n",
    " # Get player name\n",
    "try:\n",
    "    name_span = soup.find('span', {'class': 'sidearm-roster-player-name'})\n",
    "    \n",
    "    # Try to find the first name span\n",
    "    first_name_span = name_span.find('span', {'class': 'sidearm-roster-player-first-name'})\n",
    "    if first_name_span is not None:\n",
    "        first_name = first_name_span.text\n",
    "    else:\n",
    "        # Try something else if the first name span doesn't exist\n",
    "        spans = name_span.find_all('span')\n",
    "        first_name = spans[0].text if spans else ''\n",
    "    \n",
    "    # Try to find the last name span\n",
    "    last_name_span = name_span.find('span', {'class': 'sidearm-roster-player-last-name'})\n",
    "    if last_name_span is not None:\n",
    "        last_name = last_name_span.text\n",
    "    else:\n",
    "        # Try something else if the last name span doesn't exist\n",
    "        spans = name_span.find_all('span')\n",
    "        last_name = spans[1].text if len(spans) > 1 else ''\n",
    "    \n",
    "    name = f\"{first_name} {last_name}\"\n",
    "\n",
    "except AttributeError:\n",
    "    name = ''  # Handle missing name\n",
    "\n",
    "# Get jersey number\n",
    "try:\n",
    "    jersey_number_span = soup.find('span', {'class': 'sidearm-roster-player-jersey-number'})\n",
    "    jersey_number = jersey_number_span.text.strip()\n",
    "except AttributeError:\n",
    "    jersey_number = ''  # Handle missing jersey number\n",
    "\n",
    "# Get player attributes\n",
    "try:\n",
    "    ul = soup.find('ul', {'class': 'flex flex-item-1 row flex-wrap'})\n",
    "    dl_tags = ul.find_all('dl')\n",
    "    headers = [tag.find('dt').text.rstrip(':') for tag in dl_tags]  # Remove colons\n",
    "    # print(f\"headers: {headers}\")\n",
    "    data = [tag.find('dd').text for tag in dl_tags]\n",
    "except AttributeError:\n",
    "    data = '' #Handle missing player bio info\n",
    "# Get social media link\n",
    "try:\n",
    "    social_media_div = soup.find('div', {'class': 'sidearm-roster-player-social flex flex-wrap'})\n",
    "    if social_media_div is not None:\n",
    "        social_media_a = social_media_div.find('a', {'class': 'sidearm-roster-player-social-link'})\n",
    "        if social_media_a is not None:\n",
    "            social_media_link = social_media_a['href']\n",
    "        else:\n",
    "            social_media_link = ''  # Handle missing social media a tag\n",
    "    else:\n",
    "        social_media_link = ''  # Handle missing social media div\n",
    "except KeyError:\n",
    "    social_media_link = ''  # Handle missing href attribute\n",
    "\n",
    "\n",
    "# Add name, jersey number, and social media link to the list of headers and data\n",
    "# headers = ['Name', 'Jersey Number', 'Social Media'] + headers\n",
    "# data = [name, jersey_number, social_media_link] + data\n",
    "\n",
    "# Create a dictionary with the player's data\n",
    "player_data = {'Name': name, 'Jersey Number': jersey_number, 'Social Media': social_media_link}\n",
    "player_data.update(dict(zip(headers, data)))\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "player_df = pd.DataFrame([player_data])\n",
    "\n",
    "# Append the player's data to the DataFrame\n",
    "player_bio_df = pd.concat([player_bio_df, player_df], ignore_index=True, sort=False)\n",
    "\n",
    "# print(df.columns)\n",
    "print(player_bio_df)\n",
    "## NEED TO FIGURE OUT HOW TO COMBINE ROSTER STATS FROM TABLES THAT DON:T HAVE LIKE COLUMNS BUT WILL HAVE OVERLAPPING COLUMNS SOMEWHAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04203b44-aac3-40ed-bac4-4b3e38b0f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE INDIVIDUAL PLAYER BIO LINK/PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804d457-3635-4855-af1d-178a3cb70b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(player_bio_df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2722e6-035a-4007-80a1-0aa42ce718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "official_websites.to_csv(f\"data/all_schools_and_media_links_w_roster_urls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a508db-3e85-4030-9e26-4f0ec26b8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(official_websites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444db568-08ec-4f42-8b0e-2e3d9d52fb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
